{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoinW2W = torch.load('/home/francesco/Desktop/NLP/nlp2022-hw1-main/model/model_w2w.pt')\n",
    "checkpoinGLOVE = torch.load('/home/francesco/Desktop/NLP/nlp2022-hw1-main/model/model_glove300.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LSTM_Glovo(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim_Glovo, hidden_dim_Glovo):\n",
    "        super(LSTM_Glovo, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        #glovo\n",
    "        self.lstm_glovo = torch.nn.LSTM(embedding_dim_Glovo, hidden_dim_Glovo, batch_first=True,num_layers=2,bidirectional=True,dropout=0.3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, sentence_glovo):\n",
    "\n",
    "\n",
    "        lstm_out_glovo, _ = self.lstm_glovo(sentence_glovo)\n",
    "       \n",
    "\n",
    "        return lstm_out_glovo\n",
    "\n",
    "class LSTM_W2W(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,embedding_dim_w2w, hidden_dim_w2w):\n",
    "        super(LSTM_W2W, self).__init__()\n",
    "\n",
    "        #w2w\n",
    "        self.lstm_w2w = torch.nn.LSTM(embedding_dim_w2w, hidden_dim_w2w, batch_first=True,num_layers=2,bidirectional=True,dropout=0.3)\n",
    "\n",
    "\n",
    "    def forward(self, sentence_w2w):\n",
    "\n",
    "\n",
    "        lstm_out_w2w, _ = self.lstm_w2w(sentence_w2w)\n",
    "\n",
    "\n",
    "        return lstm_out_w2w\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim_glovo, embedding_dim_w2w, tagset_size):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        self.hidden_classifier_0 = torch.nn.Linear(1200,600)\n",
    "        self.hidden_classifier_1 = torch.nn.Linear(600, tagset_size*tagset_size)\n",
    "        self.hidden_classifier_2 = torch.nn.Linear(tagset_size*tagset_size, tagset_size)\n",
    "        self.drop = torch.nn.Dropout(p=0.25)\n",
    "        self.soft_max = torch.nn.Softmax(dim=2)\n",
    "\n",
    "\n",
    "    def forward(self, input_glovo, inputw2w):\n",
    "\n",
    "        input=torch.cat((input_glovo, inputw2w), 2)\n",
    "\n",
    "\n",
    "        x = self.hidden_classifier_0(input)\n",
    "        x = self.drop(x)\n",
    "        x = self.hidden_classifier_1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.hidden_classifier_2(x)\n",
    "        tag_scores=self.soft_max(x)\n",
    "    \n",
    "\n",
    "\n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "\n",
    "model_glovo=LSTM_Glovo(300,300)\n",
    "model_w2w=LSTM_W2W(300,300)\n",
    "\n",
    "'''\n",
    "for params in model_w2w.parameters():\n",
    "    print('Before loading',params)\n",
    "'''\n",
    "\n",
    "model_w2w.load_state_dict(checkpoinW2W, strict=False)\n",
    "model_glovo.load_state_dict(checkpoinGLOVE, strict=False)\n",
    "\n",
    "'''\n",
    "for params in model_w2w.parameters():\n",
    "    print('After loading',params)\n",
    "'''\n",
    "\n",
    "\n",
    "model_classifier=Classifier(300,300,13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your path for TRAIN dataset is : /home/francesco/Desktop/NLP/nlp2022-hw1-main/data/train.tsv\n",
      "\n",
      "Your path for DEV dataset is : /home/francesco/Desktop/NLP/nlp2022-hw1-main/data/dev.tsv\n"
     ]
    }
   ],
   "source": [
    "from utils import dataset_creation\n",
    "import os\n",
    "from pathlib import Path\n",
    "from utils import vocabulary_and_lableDictionary\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "#to get the current working directory\n",
    "current_dir = Path(os.getcwd())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Create Dataset for \n",
    "Train\n",
    "'''\n",
    "main_dir=current_dir.parent.parent.absolute()\n",
    "dataset_path=os.path.join(main_dir, \"data/train.tsv\")\n",
    "data_train=dataset_creation(dataset_path)\n",
    "vocab,label_dict=vocabulary_and_lableDictionary(data_train)\n",
    "\n",
    "\n",
    "'''\n",
    "Create Dataset for \n",
    "Test\n",
    "'''\n",
    "dataset_path_dev=os.path.join(main_dir, \"data/dev.tsv\")\n",
    "data_test=dataset_creation(dataset_path_dev)\n",
    "print('\\nYour path for TRAIN dataset is :',dataset_path)\n",
    "print('\\nYour path for DEV dataset is :',dataset_path_dev)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "dict_path_w2w=os.path.join(main_dir, \"hw1/data/Dict_W2W.npy\")\n",
    "weight_path_w2w=os.path.join(main_dir, \"hw1/data/Weight_W2W.npy\")\n",
    "\n",
    "Dictionary_w2w2 = np.load(dict_path_w2w,allow_pickle='TRUE').item()\n",
    "weight_w2w = np.load(weight_path_w2w,allow_pickle='TRUE')\n",
    "weight_tensor_w2w=torch.from_numpy(weight_w2w)\n",
    "\n",
    "\n",
    "dict_path_glove=os.path.join(main_dir, \"hw1/data/Dict_Glove.npy\")\n",
    "weight_path_glove=os.path.join(main_dir, \"hw1/data/Weight_Glove.npy\")\n",
    "\n",
    "Dictionary_glove = np.load(dict_path_glove,allow_pickle='TRUE').item()\n",
    "weight_glove = np.load(weight_path_glove,allow_pickle='TRUE')\n",
    "weight_tensor_glove=torch.from_numpy(weight_glove)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from typing import Dict, List\n",
    "\n",
    "# collate fn\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "\n",
    "\n",
    "def prepare_batch_glove(batch: List[Dict]) -> List[Dict]:\n",
    "  # extract features and labels from batch\n",
    "  x = [sample[\"sentence\"] for sample in batch]\n",
    "  y = [sample[\"labels\"] for sample in batch]\n",
    "  # convert words to index\n",
    "  x = [[Dictionary_glove.get(word, Dictionary_glove[UNK_TOKEN]) for word in sample] for sample in x]\n",
    "  # convert labels to index\n",
    "  y = [[label_dict.get(label) for label in sample] for sample in y]\n",
    "  # convert features to tensor and pad them\n",
    "  x = pad_sequence(\n",
    "    [torch.as_tensor(sample) for sample in x],\n",
    "    batch_first=True,\n",
    "    padding_value=Dictionary_glove.get(PAD_TOKEN)\n",
    "  )\n",
    "  # convert and pad labels too\n",
    "  y = pad_sequence(\n",
    "    [torch.as_tensor(sample) for sample in y],\n",
    "    batch_first=True,\n",
    "    padding_value=0\n",
    "  )\n",
    "  return {\"x\": x, \"y\": y}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_batch_w2w(batch: List[Dict]) -> List[Dict]:\n",
    "  # extract features and labels from batch\n",
    "  x = [sample[\"sentence\"] for sample in batch]\n",
    "  y = [sample[\"labels\"] for sample in batch]\n",
    "  # convert words to index\n",
    "  x = [[Dictionary_w2w2.get(word, Dictionary_w2w2[UNK_TOKEN]) for word in sample] for sample in x]\n",
    "  # convert labels to index\n",
    "  y = [[label_dict.get(label) for label in sample] for sample in y]\n",
    "  # convert features to tensor and pad them\n",
    "  x = pad_sequence(\n",
    "    [torch.as_tensor(sample) for sample in x],\n",
    "    batch_first=True,\n",
    "    padding_value=Dictionary_w2w2.get(PAD_TOKEN)\n",
    "  )\n",
    "  # convert and pad labels too\n",
    "  y = pad_sequence(\n",
    "    [torch.as_tensor(sample) for sample in y],\n",
    "    batch_first=True,\n",
    "    padding_value=0\n",
    "  )\n",
    "  return {\"x\": x, \"y\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "#________________________________________________________________________\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> List[Dict]:\n",
    "        return self.data[index]\n",
    "#________________________________________________________________________\n",
    "class Custom_Embedding():\n",
    "\n",
    "    def __init__(self, weights, dict, batch, by_id ) -> None:\n",
    "\n",
    "        self.weights = weights\n",
    "        self.dict = dict\n",
    "        self.batch = batch\n",
    "        self.weights_leng = len(self.weights[0,:])\n",
    "        self.by_id = by_id\n",
    "    \n",
    "\n",
    "    def get_weight(self,token):\n",
    "\n",
    "\n",
    "        if(self.by_id == False):\n",
    "            try : index_token = self.dict[token]\n",
    "            except : \n",
    "                #print('missing')\n",
    "                index_token = self.dict['UNK']\n",
    "        else:\n",
    "            return self.weights[token,:]\n",
    "\n",
    "        \n",
    "    \n",
    "        return self.weights[index_token,:]\n",
    "\n",
    "\n",
    "    def embedd(self,list_words):\n",
    "\n",
    "        list=[]\n",
    "        \n",
    "        if(self.batch == False):\n",
    "\n",
    "            for word in list_words:\n",
    "                list.append(self.get_weight(word).tolist())\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            return torch.tensor(list)\n",
    "        \n",
    "        elif(self.batch == True):\n",
    "\n",
    "            max_pad = longest(list_words)\n",
    "            #print('max padd',max_pad)\n",
    "\n",
    "            \n",
    "\n",
    "            for sentence in list_words:\n",
    "\n",
    "                current_lenght = len(sentence)\n",
    "                #print('current :',current_lenght)\n",
    "                sencence=[]\n",
    "                for word in sentence:\n",
    "                    sencence.append(self.get_weight(word).tolist())\n",
    "                \n",
    "                \n",
    "\n",
    "                if(max_pad>current_lenght):\n",
    "                    #print('padding')\n",
    "                    listofzeros = [0] * self.weights_leng\n",
    "\n",
    "                    while max_pad>current_lenght:\n",
    "                        #print('padding now')\n",
    "                        sencence.append(listofzeros)\n",
    "                        #print('new leng',len(sencence))\n",
    "                        current_lenght = len(sencence)\n",
    "                \n",
    "\n",
    "                #print('append list',len(sencence))\n",
    "                list.append(sencence)\n",
    "\n",
    "\n",
    "            return torch.tensor(list)\n",
    "#________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def longest(list1):\n",
    "    longest_list = max(len(elem) for elem in list1)\n",
    "    return longest_list\n",
    "\n",
    "\n",
    "\n",
    "# data loader parameters\n",
    "collate_fn_w2w = prepare_batch_w2w # the function that will prepare the data for the model\n",
    "collate_fn_glove = prepare_batch_glove # the function that will prepare the data for the model\n",
    "batch_sizes = 32\n",
    "num_workers = min(os.cpu_count(), 4)  # it is usually 4 workers per GPU\n",
    "is_train_dataloader = False # we don\"t want to shuffle dev and test data\n",
    "\n",
    "\n",
    "\n",
    "#NERDdataset for both training and test(from dev.tsv)\n",
    "train_dataset = NERDataset(data_train)\n",
    "dev_dataset=NERDataset(data_test)\n",
    "\n",
    "\n",
    "\n",
    "#DataLoader definition\n",
    "\n",
    "train_data_loader_glove = DataLoader(\n",
    "  train_dataset,\n",
    "  collate_fn=collate_fn_glove,\n",
    "  shuffle=is_train_dataloader,\n",
    "  batch_size=batch_sizes,\n",
    "  num_workers=num_workers,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader_w2w = DataLoader(\n",
    "  train_dataset,\n",
    "  collate_fn=collate_fn_w2w,\n",
    "  shuffle=is_train_dataloader,\n",
    "  batch_size=batch_sizes,\n",
    "  num_workers=num_workers,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#collate_fn_w2w or collate_fn_glove\n",
    "dev_data_loader = DataLoader(\n",
    "  dev_dataset,\n",
    "  collate_fn=collate_fn_w2w,\n",
    "  shuffle=is_train_dataloader,\n",
    "  batch_size=batch_sizes,\n",
    "  num_workers=num_workers,\n",
    ")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EPOCHS n.0\n",
      "______________________TRAIN_______________________________\n",
      "f1 score : [0.97268594 0.0029344  0.00740349 0.49644296 0.41487802 0.56256499\n",
      " 0.00268962 0.00254858 0.01365188 0.44746334 0.00166113 0.00216998\n",
      " 0.        ]\n",
      "accuracy training : [0.911157]\n",
      "f1 score macro:  0.22516110218738886\n",
      "______________________VALIDATION_______________________________\n",
      "f1 score : [0.9845597  0.00306816 0.         0.66948872 0.71006284 0.87801484\n",
      " 0.         0.         0.         0.63213315 0.         0.\n",
      " 0.        ]\n",
      "accuracy training : [0.9392249]\n",
      "f1 score macro:  0.29825595506779085\n",
      "\n",
      "\n",
      "EPOCHS n.1\n",
      "______________________TRAIN_______________________________\n",
      "f1 score : [0.98900533 0.56817915 0.         0.70744644 0.80180783 0.91139038\n",
      " 0.         0.         0.60428208 0.71784745 0.         0.\n",
      " 0.        ]\n",
      "accuracy training : [0.95029091]\n",
      "f1 score macro:  0.40768912752039976\n",
      "______________________VALIDATION_______________________________\n",
      "f1 score : [0.99189407 0.70136329 0.         0.74139367 0.86368863 0.93451647\n",
      " 0.         0.         0.7313469  0.80346663 0.         0.\n",
      " 0.        ]\n",
      "accuracy training : [0.95743308]\n",
      "f1 score macro:  0.44366689711393437\n",
      "\n",
      "\n",
      "EPOCHS n.2\n",
      "______________________TRAIN_______________________________\n",
      "f1 score : [0.99203364 0.71483278 0.         0.76766091 0.86743796 0.92974467\n",
      " 0.         0.         0.71517428 0.80809462 0.         0.\n",
      " 0.        ]\n",
      "accuracy training : [0.95806325]\n",
      "f1 score macro:  0.44576760474157595\n",
      "______________________VALIDATION_______________________________\n",
      "f1 score : [0.99267944 0.71209484 0.         0.77650572 0.86675992 0.93335509\n",
      " 0.         0.         0.72052148 0.82319444 0.         0.\n",
      " 0.        ]\n",
      "accuracy training : [0.95898489]\n",
      "f1 score macro:  0.4480854557725425\n",
      "\n",
      "\n",
      "EPOCHS n.3\n",
      "______________________TRAIN_______________________________\n",
      "f1 score : [0.99303958 0.82385816 0.62715613 0.84667656 0.91036907 0.95215869\n",
      " 0.         0.00133824 0.76359545 0.85716354 0.         0.\n",
      " 0.70436708]\n",
      "accuracy training : [0.96709181]\n",
      "f1 score macro:  0.5753632690521121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_classifier.parameters(), lr=0.1, momentum=0.9)\n",
    "emb_glove = Custom_Embedding(weight_tensor_glove,Dictionary_glove,True,True)\n",
    "emb_w2w = Custom_Embedding(weight_tensor_w2w,Dictionary_w2w2,True,True)\n",
    "\n",
    "\n",
    "i=0\n",
    "loss_history = []\n",
    "loss_history_val = []\n",
    "\n",
    "\n",
    "f1_history = []\n",
    "f1_history_val = []\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "  count=0\n",
    "  accuracy_epoch=0\n",
    "  total_prediction=[]\n",
    "  total_labels=[]\n",
    "  running_loss = 0\n",
    "  model_classifier.train()\n",
    "  \n",
    "\n",
    "  for ext_counter,batch in enumerate(train_data_loader_glove):\n",
    "    for in_counter,batch_ in enumerate(train_data_loader_w2w):\n",
    "      \n",
    "\n",
    "\n",
    "      if(ext_counter==in_counter):\n",
    "\n",
    "        model_classifier.zero_grad()\n",
    "\n",
    "        batch_x_g = batch['x']\n",
    "        batch_x_w2w = batch_['x']\n",
    "\n",
    "        \n",
    "        batch_y = batch['y']\n",
    "        #or is the same\n",
    "        #batch_y = batch_['y']\n",
    "\n",
    "        #GLOVE EMBEDDING\n",
    "        batch_x_glove = emb_glove.embedd(batch_x_g.tolist())\n",
    "        batch_x_w2w = emb_w2w.embedd(batch_x_w2w.tolist())\n",
    "        #print(batch_x.size())\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        prediction_lstm_glovo=model_glovo.forward(batch_x_glove)\n",
    "        prediction_lstm_w2w=model_w2w.forward(batch_x_w2w)\n",
    "\n",
    "\n",
    "        prediction = model_classifier.forward(prediction_lstm_glovo,prediction_lstm_w2w)\n",
    "\n",
    "        #print('Predicted: ',prediction.size())\n",
    "        #print('Ground Truth: ',batch_y.size())\n",
    "        sizes=prediction.size()\n",
    "        #print(sizes[0])\n",
    "        l0=sizes[0]\n",
    "        l1=sizes[1]\n",
    "        l2=sizes[2]\n",
    "        l3=l0*l1\n",
    "\n",
    "        new_size = (l3,l2)\n",
    "        prediction_batch = torch.reshape(prediction,new_size)\n",
    "\n",
    "\n",
    "        \n",
    "        target_batch = torch.flatten(batch_y)\n",
    "      \n",
    "\n",
    "        \n",
    "\n",
    "        loss = loss_function(prediction_batch,target_batch)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pre_accu=torch.argmax(prediction_batch, dim=1)\n",
    "\n",
    "        #print('predited',pre_accu)\n",
    "        #print('target',target_batch)\n",
    "\n",
    "\n",
    "        train_acc = torch.sum(pre_accu == target_batch)\n",
    "\n",
    "        batch_train_acc = (np.array(train_acc))/(target_batch.size())\n",
    "\n",
    "        accuracy_epoch=accuracy_epoch+batch_train_acc\n",
    "        \n",
    "\n",
    "        \n",
    "        total_prediction=total_prediction+(pre_accu.tolist())\n",
    "        total_labels=total_labels+(target_batch.tolist())\n",
    "        count+=1\n",
    "\n",
    "\n",
    "      else:\n",
    "        pass\n",
    "\n",
    "  \n",
    "\n",
    "  running_loss_avg = (running_loss/count)\n",
    "  loss_history.append(running_loss_avg)\n",
    "  f1_score_epoch = f1_score(total_labels, total_prediction,average=None)\n",
    "  f1_score_epoch_macro = f1_score(total_labels, total_prediction,average='macro')\n",
    "  f1_history.append(f1_score_epoch_macro)\n",
    "  print('\\n\\nEPOCHS n.'+str(epoch))\n",
    "  print('______________________TRAIN_______________________________')\n",
    "  print('f1 score :',f1_score_epoch)\n",
    "  print('accuracy training :',accuracy_epoch/count)\n",
    "  print('f1 score macro: ',f1_score_epoch_macro)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  '''\n",
    "  validation on this epoch\n",
    "  \n",
    "  '''\n",
    "  model_classifier.eval()\n",
    "  with torch.no_grad():\n",
    "\n",
    "    count_val=0\n",
    "    accuracy_epoch_val=0\n",
    "    total_prediction_val=[]\n",
    "    total_labels_val=[]\n",
    "    running_loss_val = 0\n",
    "\n",
    "    for ext_counter,batch in enumerate(train_data_loader_glove):\n",
    "      for in_counter,batch_ in enumerate(train_data_loader_w2w):\n",
    "      \n",
    "\n",
    "\n",
    "        if(ext_counter==in_counter):\n",
    "\n",
    "\n",
    "          batch_x_g = batch['x']\n",
    "          batch_x_w2w = batch_['x']\n",
    "\n",
    "          \n",
    "          batch_y = batch['y']\n",
    "          #or is the same\n",
    "          #batch_y = batch_['y']\n",
    "\n",
    "          #GLOVE EMBEDDING\n",
    "          batch_x_glove = emb_glove.embedd(batch_x_g.tolist())\n",
    "          batch_x_w2w = emb_w2w.embedd(batch_x_w2w.tolist())\n",
    "          #print(batch_x.size())\n",
    "\n",
    "          \n",
    "          \n",
    "          \n",
    "\n",
    "          prediction_lstm_glovo=model_glovo.forward(batch_x_glove)\n",
    "          prediction_lstm_w2w=model_w2w.forward(batch_x_w2w)\n",
    "\n",
    "\n",
    "          prediction = model_classifier.forward(prediction_lstm_glovo,prediction_lstm_w2w)\n",
    "\n",
    "\n",
    "          sizes=prediction.size()\n",
    "          l0=sizes[0]\n",
    "          l1=sizes[1]\n",
    "          l2=sizes[2]\n",
    "          l3=l0*l1\n",
    "          new_size = (l3,l2)\n",
    "          prediction_batch = torch.reshape(prediction,new_size)\n",
    "          target_batch = torch.flatten(batch_y)\n",
    "        \n",
    "\n",
    "          \n",
    "\n",
    "          loss = loss_function(prediction_batch,target_batch)\n",
    "          running_loss_val += loss.item()\n",
    "\n",
    "\n",
    "          pre_accu=torch.argmax(prediction_batch, dim=1)\n",
    "          train_acc = torch.sum(pre_accu == target_batch)\n",
    "          batch_train_acc = (np.array(train_acc))/(target_batch.size())\n",
    "\n",
    "          accuracy_epoch_val=accuracy_epoch_val+batch_train_acc\n",
    "          \n",
    "\n",
    "          \n",
    "          total_prediction_val=total_prediction_val+(pre_accu.tolist())\n",
    "          total_labels_val=total_labels_val+(target_batch.tolist())\n",
    "\n",
    "        \n",
    "          \n",
    "          count_val+=1\n",
    "        \n",
    "\n",
    "        else:\n",
    "          pass\n",
    "    \n",
    "\n",
    "    running_loss_val_avg = (running_loss_val/count_val)\n",
    "    loss_history_val.append(running_loss_val_avg)\n",
    "    f1_score_epoch_val = f1_score(total_labels_val, total_prediction_val,average=None)\n",
    "    f1_score_epoch_val_macro = f1_score(total_labels_val, total_prediction_val,average='macro')\n",
    "    f1_history_val.append(f1_score_epoch_val_macro)\n",
    "    print('______________________VALIDATION_______________________________')\n",
    "    print('f1 score :',f1_score_epoch_val)\n",
    "    print('accuracy training :',accuracy_epoch_val/count_val)\n",
    "    print('f1 score macro: ',f1_score_epoch_val_macro)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_path=os.path.join(main_dir, \"model/model_classifier.pt\")\n",
    "torch.save(model_classifier.state_dict(),model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/francesco/Desktop/NLP/nlp2022-hw1-main/hw1/stud/load.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/NLP/nlp2022-hw1-main/hw1/stud/load.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39m#Plot accuracy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/NLP/nlp2022-hw1-main/hw1/stud/load.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39m#NB: my accuracy is \"drogata\" dalla presenza del padding \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/NLP/nlp2022-hw1-main/hw1/stud/load.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39m#Padding elements are classified as 0,  class 0 is then more populated and because is accuracy is always above 0.9 tha final accuracy results more high then reality\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/NLP/nlp2022-hw1-main/hw1/stud/load.ipynb#ch0000009?line=3'>4</a>\u001b[0m \u001b[39m#In order to check is the model is Overfitting and limit the effect of padding on loss/accuracy results i am using the f_1 results on both train and \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/francesco/Desktop/NLP/nlp2022-hw1-main/hw1/stud/load.ipynb#ch0000009?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/NLP/nlp2022-hw1-main/hw1/stud/load.ipynb#ch0000009?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m111\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/NLP/nlp2022-hw1-main/hw1/stud/load.ipynb#ch0000009?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mLoss Evolution\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#Plot accuracy\n",
    "#NB: my accuracy is \"drogata\" dalla presenza del padding \n",
    "#Padding elements are classified as 0,  class 0 is then more populated and because is accuracy is always above 0.9 tha final accuracy results more high then reality\n",
    "#In order to check is the model is Overfitting and limit the effect of padding on loss/accuracy results i am using the f_1 results on both train and \n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(111)\n",
    "plt.title('Loss Evolution')\n",
    "plt.plot(np.array(loss_history), 'r',np.array(loss_history_val),'b')\n",
    "plt.ylabel('CE LOSS')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(111)\n",
    "plt.title('F1 Evolution')\n",
    "plt.plot(np.array(f1_history), 'r',np.array(f1_history_val),'b')\n",
    "plt.ylabel('F1 multiclass score(unweighted)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "list_1 = [1,2,3,4]\n",
    "list_2 = [1,2,3,4]\n",
    "\n",
    "for ext_count, l1 in enumerate(list_1):\n",
    "    for in_count,l2 in enumerate(list_2):\n",
    "        if(ext_count==in_count):\n",
    "            print(l1)\n",
    "            print(l2)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82c5ea8f03e0c61b8a1eddf7dd4fb0dcc59038309c931ef184c5056600baac5b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp2022-hw1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
