{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILES PRE-PROCESSING FOR GLOVE EMBEDDING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary and Weight size torch.Size([400002, 50])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_GLOVE(PATH):\n",
    "\n",
    "    Dictionary = {}\n",
    "    weights = []\n",
    "\n",
    "    count=0\n",
    "    with open(PATH, 'r', encoding=\"utf-8\") as f:\n",
    "        \n",
    "            content = f.readlines()\n",
    "            for row in content:\n",
    "\n",
    "                vector = []\n",
    "                \n",
    "                \n",
    "                s = row.split(' ')\n",
    "                Dictionary[s[0]] = count\n",
    "                count +=1\n",
    "                values = s[1:]\n",
    "                for v in values:\n",
    "                    v = v.rstrip()\n",
    "                    #print(type(float(v)))\n",
    "\n",
    "                    vector.append(float(v))\n",
    "                    #print(type(vector[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                weights.append(vector)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "            #count+=1\n",
    "            #Dictionary[line]\n",
    "\n",
    "    return torch.tensor(weights),Dictionary\n",
    "    #print(weight_tensor.size())\n",
    "\n",
    "\n",
    "\n",
    "path=\"data/glove.6B.200d.txt\"\n",
    "weight_tensor,Dictionary = load_GLOVE(path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Dictionary and Weight size',(weight_tensor.size()))\n",
    "#Save Dictionary and Glove pre-trained weights\n",
    "np.save('Dict_Glove200.npy', Dictionary)\n",
    "np.save('Weight_Glove200.npy', weight_tensor.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMEBEDDING CLASS same for GLOVE and Word2vec and test on it and it's 4 modalities\n",
    "\n",
    "**batch** that can be used to spefify if is expecting only a tokenized sentence or a batch ot tokenized senteces\n",
    "\n",
    "**by_id** to specify if the model should exspect list of tokens or already indexed senteces. \n",
    "\n",
    "ie. for bath = False and [0 1 4 58 15 98] with by_id=True and ['the','cat','is','on','the','table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([100])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 5, 100])\n",
      "torch.Size([3, 6, 100])\n"
     ]
    }
   ],
   "source": [
    "class GloveEmbedding():\n",
    "\n",
    "    def __init__(self, weights, dict, batch, by_id ) -> None:\n",
    "\n",
    "        self.weights = weights\n",
    "        self.dict = dict\n",
    "        self.batch = batch\n",
    "        self.weights_leng = len(self.weights[0,:])\n",
    "        self.by_id = by_id\n",
    "    \n",
    "\n",
    "    def get_weight(self,token):\n",
    "\n",
    "        if(self.by_id == False):\n",
    "            index_token = self.dict[token]\n",
    "        else:\n",
    "            return self.weights[token,:]\n",
    "\n",
    "        \n",
    "    \n",
    "        return self.weights[index_token,:]\n",
    "\n",
    "\n",
    "    def embedd(self,list_words):\n",
    "\n",
    "        list=[]\n",
    "        \n",
    "        if(self.batch == False):\n",
    "\n",
    "            for word in list_words:\n",
    "                list.append(self.get_weight(word).tolist())\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            return torch.tensor(list)\n",
    "        \n",
    "        elif(self.batch == True):\n",
    "\n",
    "            max_pad = longest(list_words)\n",
    "            #print('max padd',max_pad)\n",
    "\n",
    "            \n",
    "\n",
    "            for sentence in list_words:\n",
    "\n",
    "                current_lenght = len(sentence)\n",
    "                #print('current :',current_lenght)\n",
    "                sencence=[]\n",
    "                for word in sentence:\n",
    "                    sencence.append(self.get_weight(word).tolist())\n",
    "                \n",
    "                \n",
    "\n",
    "                if(max_pad>current_lenght):\n",
    "                    #print('padding')\n",
    "                    listofzeros = [0] * self.weights_leng\n",
    "\n",
    "                    while max_pad>current_lenght:\n",
    "                        #print('padding now')\n",
    "                        sencence.append(listofzeros)\n",
    "                        #print('new leng',len(sencence))\n",
    "                        current_lenght = len(sencence)\n",
    "                \n",
    "\n",
    "                #print('append list',len(sencence))\n",
    "                list.append(sencence)\n",
    "\n",
    "\n",
    "            return torch.tensor(list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def longest(list1):\n",
    "    longest_list = max(len(elem) for elem in list1)\n",
    "    return longest_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TESTING GLOVE EMBEDDING\n",
    "\n",
    "lis=[['table','on','book','you','?'],['the','my','cat'],['table','on','book','you']]\n",
    "#CHECK on longest fucntion,  used for padding in batch mode\n",
    "print(longest(lis))\n",
    "\n",
    "Ge = GloveEmbedding(weight_tensor,Dictionary,False,False)\n",
    "#Check on get_weight function\n",
    "print(Ge.get_weight('robin').size())\n",
    "#Using BATCH = False\n",
    "print(Ge.embedd(['the','my','cat']).size())\n",
    "\n",
    "\n",
    "\n",
    "#Using BATCH = TRUE but passing no index but directly words\n",
    "Ge = GloveEmbedding(weight_tensor,Dictionary,True,False)\n",
    "print(Ge.embedd([['the','my','cat'],['table','on','book','you'],['table','on','book','you','?']]).size())\n",
    "\n",
    "#Using BATCH = TRUE but with passing idexes of words\n",
    "Ge = GloveEmbedding(weight_tensor,Dictionary,True,True)\n",
    "print(Ge.embedd([[1,0,3,5],[0,2],[1,4,8,9,2,5]]).size())\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILES PRE-PROCESSING FOR WORD2VEC EMBEDDING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model load_word2vec_format Created\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def zerolistmaker(n):\n",
    "    listofzeros = [0] * n\n",
    "    return listofzeros\n",
    "\n",
    "\n",
    "\n",
    "def load_W2W(PATH):\n",
    "\n",
    "    Dictionary = {}\n",
    "    weights = []\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(PATH, binary=True)\n",
    "    print('Model load_word2vec_format Created')\n",
    "    count=0\n",
    "\n",
    "    for index, word in enumerate(model.index_to_key):\n",
    "\n",
    "        Dictionary[word] = index\n",
    "        weights.append(model[word])\n",
    "        count+=1\n",
    "\n",
    "\n",
    "    #ADD PAD AND UNK \n",
    "    pad = '<PAD>'\n",
    "    pad_w = zerolistmaker(300)\n",
    "    un = '<UNK>'\n",
    "    un_w = zerolistmaker(300)\n",
    "\n",
    "    Dictionary[pad] = count\n",
    "    weights.append(pad_w)\n",
    "\n",
    "    Dictionary[un] = count+1\n",
    "    weights.append(un_w)\n",
    "        \n",
    "\n",
    "    return torch.tensor(weights),Dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path=\"GoogleNews-vectors-negative300.bin\"\n",
    "weight_tensor,Dictionary = load_W2W(path)\n",
    "\n",
    "\n",
    "#Save Dictionary and Word2Vec pre-trained weights\n",
    "print('Dictionary and Weight size :',weight_tensor.size())\n",
    "np.save('Dict_W2W.npy', Dictionary)\n",
    "np.save('Weight_W2W.npy', weight_tensor.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82c5ea8f03e0c61b8a1eddf7dd4fb0dcc59038309c931ef184c5056600baac5b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp2022-hw1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
